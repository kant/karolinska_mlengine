{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Estimators\n",
    "Estimators are a high-level wrapper which takes care of many previously tedious things of a Tenserflow implementation. They take care of seperating your model from \"training-only\" parts of the graph, making it alot cleaner, start queue runners, switching inputs and model modes and much more. I highly reccomend taking the time and get familiar with using TF Estimators.\n",
    "\n",
    "<img style=\"float: left\" width=500 src=\"./data/estimators.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the estimators, we need to create the desired architecture, the model method and the input method.<br>\n",
    "Note, here a model means the model for a specific task, not the architecture. Thus the same model method can be used for multiple architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture\n",
    "Let us start by creating a simple architecture to use. This arcitecture will take in a tensor (batch of images) and output a 2 channel tensor, one for each pixel class. This simple architecture is going to need a de-convolution layer, which, according to this https://distill.pub/2016/deconv-checkerboard/ should be done in a specific way to achieve good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def deconv2d_resize(inputs,\n",
    "                    filters,\n",
    "                    kernel_size=(2, 2),\n",
    "                    padding='SAME',\n",
    "                    strides=(2, 2),\n",
    "                    reuse=None,\n",
    "                    name=None,\n",
    "                    activation=None):\n",
    "    \"\"\"Resize input using nearest neighbor then apply convolution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs : tensor\n",
    "        The input tensor to this operation\n",
    "    filters : int\n",
    "        Number of filters of the conv operation\n",
    "    kernel_size : tuple, optional\n",
    "        The kernel size to use\n",
    "    padding : str, optional\n",
    "        Padding strategy\n",
    "    strides : tuple, optional\n",
    "        How many steps the resize operation should take, the strides\n",
    "        control how big the output tensor is\n",
    "    reuse : None, optional\n",
    "        Variable to control if the generated weights should be reused from somewhere else\n",
    "    name : None, optional\n",
    "        Desired name of this op\n",
    "    activation : None, optional\n",
    "        Desired activation function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        The output tensor that has been resized and convolved\n",
    "    \"\"\"\n",
    "    shape = inputs.get_shape().as_list()\n",
    "    height = shape[1] * strides[0]\n",
    "    width = shape[2] * strides[1]\n",
    "    resized = tf.image.resize_nearest_neighbor(inputs, [height, width])\n",
    "\n",
    "    return tf.layers.conv2d(resized, filters,\n",
    "                            kernel_size=(3, 3),\n",
    "                            padding='SAME',\n",
    "                            strides=(1, 1),\n",
    "                            reuse=reuse,\n",
    "                            name=name,\n",
    "                            activation=activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that that is finished, we can define the architecture. When creating this architecture, the inputs are generally yours to control (except for the input tensor of course). But if you are using things like dropout, batchnormalization etc, you need to include the \"mode\" to determine which mode they should be in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple(features, mode, hparams, scope='simple_network'):\n",
    "    \"\"\"Returns a simple network architecture.\n",
    "\n",
    "    conv[5,5,32] -> Dense -> Dropout -> deconv[5,5,2]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : Tensor\n",
    "        4D Tensor where the first dimension is the batch size, then height, width\n",
    "        and channels\n",
    "    mode : tensorflow.python.estimator.model_fn.ModeKeys\n",
    "        Class that contains the current mode\n",
    "    scope : str, optional\n",
    "        The scope to use for this architecture\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor op\n",
    "        Return the final tensor operation (logits), from the network\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        is_training = mode == Modes.TRAIN\n",
    "\n",
    "        # Input Layer\n",
    "        net = [features]\n",
    "        # Convolutional Layer #1\n",
    "        net.append(tf.layers.conv2d(inputs=net[-1],\n",
    "                                    filters=32,\n",
    "                                    kernel_size=[5, 5],\n",
    "                                    strides=(2, 2),\n",
    "                                    padding=\"same\",\n",
    "                                    name=\"conv_1_1\",\n",
    "                                    activation=tf.nn.relu))\n",
    "\n",
    "        # Fully connected layer\n",
    "        net.append(tf.layers.dense(inputs=net[-1], units=16, activation=tf.nn.relu))\n",
    "        \n",
    "        # Batch normaliation\n",
    "        net.append(tf.layers.batch_normalization(net[-1], training=is_training))\n",
    "\n",
    "        # Dropout\n",
    "        net.append(tf.layers.dropout(inputs=net[-1], rate=0.4, training=is_training))\n",
    "        # Deconv\n",
    "        net.append(deconv2d_resize(inputs=net[-1],\n",
    "                                   filters=2,\n",
    "                                   kernel_size=[5, 5],\n",
    "                                   padding=\"same\",\n",
    "                                   activation=tf.nn.relu))\n",
    "\n",
    "        return net[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make sure that if we put in an image of size (B, H, W, C) the output will become (B, H, W, 2) where B is the batch-size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
    "\n",
    "# Lets reset our graph, get a clean slate. This way we can run this cell multiple times..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_size = (5, 100, 100, 3)\n",
    "expected_output = (5, 100, 100, 2)\n",
    "input_tensor = tf.placeholder(shape=input_size, dtype=tf.float32)\n",
    "output_tensor = simple(input_tensor, \"train\", [])\n",
    "\n",
    "assert output_tensor.get_shape() == expected_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the architecture is ready, we move on to the model function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model function\n",
    "The model will take care of creating the \"outer layer\" of our architecture, that is, how the input and output are handled. It also creates the necessary nodes for training, evaluating, predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \"\"\"Creates the model function.\n",
    "\n",
    "    This will handle all the different processes needed when using an Estimator.\n",
    "    The estimator will change \"modes\" using the mode flag, and depending on that\n",
    "    different outputs are provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : Tensor\n",
    "        4D Tensor where the first dimension is the batch size, then height, width\n",
    "        and channels\n",
    "    labels : Dict {'label': Tensor, 'weight': Tensor}\n",
    "        Contains both weight and label, where each is a 3D Tensor, where the first dimension is\n",
    "        the batch size, then height and width. The values in the label image is class number, while\n",
    "        weight is a weight map for the pixels\n",
    "    mode : tensorflow.python.estimator.model_fn.ModeKeys\n",
    "        Class that contains the current mode\n",
    "    params : class\n",
    "        Contains all the hyper parameters that are available to the model. These can be different\n",
    "        depending on which architecture (model type) is in use\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.estimator.EstimatorSpec\n",
    "        The requested estimator spec\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch the input tensor\n",
    "    feature_input = features['inputs']\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = simple(feature_input, mode, params)\n",
    "\n",
    "    # If this is a prediction or evaluation mode, then we return\n",
    "    # the class probabilities and the guessed pixel class\n",
    "    if mode in (Modes.TRAIN, Modes.EVAL, Modes.PREDICT):\n",
    "        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n",
    "        predicted_pixels = tf.argmax(input=logits, axis=-1)\n",
    "\n",
    "    # During training and evaluation, we calculate the loss\n",
    "    if mode in (Modes.TRAIN, Modes.EVAL):\n",
    "        # The global step is needed into the optimizer\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        label_indices = tf.cast(labels['label'], tf.int32)\n",
    "        softmax = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels['label'],\n",
    "                                                                 logits=logits)\n",
    "        weighted_softmax = tf.multiply(softmax, labels['weight'])\n",
    "        # If the weights had any L2 losses, we would collect them like this\n",
    "        reg_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "        loss = tf.reduce_sum(weighted_softmax) + reg_loss\n",
    "\n",
    "    # In training (not evaluation) we perform backprop\n",
    "    if mode == Modes.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate)\n",
    "\n",
    "        # For batch normalization, we need to tie the \"Update operations\" to the\n",
    "        # calling of training_op. This updates the moving mean/variance of the \n",
    "        # batchnorm when the training op is called. This is important!!\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op)\n",
    "\n",
    "    # For evaluations, we generally just state which metric ops to use. In this\n",
    "    # case, the mean intersection over union is of interest\n",
    "    if mode == Modes.EVAL:\n",
    "        # Accuracy operations\n",
    "        eval_metric_ops = {\n",
    "            'accuracy': tf.metrics.mean_iou(label_indices, predicted_pixels, 2)\n",
    "        }\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "      \n",
    "    # When predicting (running inference only, during serving for example) we\n",
    "    # need to return the output as a dictionary.\n",
    "    if mode == Modes.PREDICT:\n",
    "        predictions = {\n",
    "            'classes': predicted_pixels,\n",
    "            'probabilities': probabilities\n",
    "        }\n",
    "        export_outputs = {\n",
    "            'prediction': tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, predictions=predictions, export_outputs=export_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the model function is split up into train, evaluation and prediction. The estimator will indicated to the model function (via mode) which state its trying to create. This way there are no \"evaluation\" operations on the graph when training and vice versa. There is no speed penalty to having this many if statements, since these operations only create the graph __creation__ but don't actually run the operations.<br>\n",
    "\n",
    "As before, lets make sure the method works as expected. Note, this time the input is not directly a tensor, rather a dictionary of tensors. The reason is that if we ever intend on serving this model somewhere, it would have to receive a dictionary to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train', EstimatorSpec(mode='train', predictions={}, loss=<tf.Tensor 'add:0' shape=() dtype=float32>, train_op=<tf.Operation 'Adam' type=AssignAdd>, eval_metric_ops={}, export_outputs=None, training_chief_hooks=(), training_hooks=(), scaffold=<tensorflow.python.training.monitored_session.Scaffold object at 0x7fc57ad6c250>, evaluation_hooks=()))\n",
      "('eval', EstimatorSpec(mode='eval', predictions={}, loss=<tf.Tensor 'add:0' shape=() dtype=float32>, train_op=None, eval_metric_ops={'accuracy': (<tf.Tensor 'mean_iou/Select_1:0' shape=() dtype=float32>, <tf.Tensor 'mean_iou/AssignAdd:0' shape=(2, 2) dtype=float64_ref>)}, export_outputs=None, training_chief_hooks=(), training_hooks=(), scaffold=<tensorflow.python.training.monitored_session.Scaffold object at 0x7fc57ae4ead0>, evaluation_hooks=()))\n",
      "('infer', EstimatorSpec(mode='infer', predictions={'probabilities': <tf.Tensor 'softmax_tensor:0' shape=(5, 100, 100, 2) dtype=float32>, 'classes': <tf.Tensor 'ArgMax:0' shape=(5, 100, 100) dtype=int64>}, loss=None, train_op=None, eval_metric_ops={}, export_outputs={'serving_default': <tensorflow.python.estimator.export.export_output.PredictOutput object at 0x7fc58d775a50>, 'prediction': <tensorflow.python.estimator.export.export_output.PredictOutput object at 0x7fc58d775a50>}, training_chief_hooks=(), training_hooks=(), scaffold=<tensorflow.python.training.monitored_session.Scaffold object at 0x7fc57ae4e650>, evaluation_hooks=()))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
    "from tensorflow.contrib.training import HParams\n",
    "\n",
    "# Modes to loop through\n",
    "test_modes = [Modes.TRAIN, Modes.EVAL, Modes.PREDICT]\n",
    "    \n",
    "for imode in test_modes:\n",
    "  \n",
    "  # Lets reset our graph, get a clean slate. This way we can run this cell multiple times..\n",
    "  tf.reset_default_graph()\n",
    "\n",
    "  input_size = (5, 100, 100, 3)\n",
    "  label_size = (5, 100, 100)\n",
    "  weight_size = (5, 100, 100)\n",
    "  expected_output = (5, 100, 100, 2)\n",
    "  input_tensor = tf.placeholder(shape=input_size, dtype=tf.float32)\n",
    "  label_tensor = tf.placeholder(shape=label_size, dtype=tf.int32)\n",
    "  weight_tensor = tf.placeholder(shape=weight_size, dtype=tf.float32)\n",
    "\n",
    "  feature_dict = {\n",
    "    'inputs': input_tensor\n",
    "  }\n",
    "  label_dict = {\n",
    "    'label': label_tensor,\n",
    "    'weight': weight_tensor\n",
    "  }\n",
    "\n",
    "\n",
    "  # Define model and input parameters\n",
    "  hparams = HParams(\n",
    "      learning_rate=0.001\n",
    "  )\n",
    "\n",
    "  response = model_fn(feature_dict, label_dict, imode, hparams)\n",
    "  print(imode, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, no run errors found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Input method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already covered the \"feeder\" in WP2. This is actually the input method to our estimator, the only thing that matters is that the input when evaluating should not be the same as when running training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
